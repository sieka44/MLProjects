{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x*(x-1)*(x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rxs = np.random.uniform(-1,1,8)\n",
    "rys = func(rxs)+np.random.normal(0,0.05,len(rxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "That's how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(rxs,rys, alpha=0.7, color='none', edgecolor=\"black\");\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To find the mapping corresponding to this data we will use the MSE loss. Untill now all was rather abstract, we were talking about optimizing over a space of all possible functions. That's obviously not possible. The way to proceed is to take a familly of functions parametrized with some set of parameters and optimize over the space of those parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Model capacity, underfitting and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A popular familly of functions are the polynomials of a given degree, the parameters being the polynomial coefficients.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will be not fitting the polynomials ourself. Instead we will use a  function `polyfit` from numpy that performs the optimization with respect to MSE loss. Then we will use function `polyval` to calculate the predictons and error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_errors = []\n",
    "polys  = []\n",
    "for deg in range(8):\n",
    "    p = np.polyfit(rxs, rys,deg)\n",
    "    polys.append(p)\n",
    "    pred_rys = np.polyval(p, rxs) \n",
    "    resid = pred_rys-rys\n",
    "    err_train= 0.5*np.dot(resid, resid)/len(rxs)\n",
    "    train_errors.append(np.array([deg, err_train]))\n",
    "fit_res_train = np.stack(train_errors,axis=0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And here is how the MSE looks as a function of the polynomial degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.grid()\n",
    "plt.scatter(fit_res_train[:,0], fit_res_train[:,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rxs_valid = np.random.uniform(-1,1,8)\n",
    "rys_valid = func(rxs_valid)+np.random.normal(0,0.05, len(rxs_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "And check how the model performs on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "validation_errors =[]\n",
    "for deg in range(8):\n",
    "    p = polys[deg]\n",
    "    pred_ys = np.polyval(p, rxs_valid) \n",
    "    resid = pred_ys-rys_valid\n",
    "    err_valid= 0.5*np.dot(resid, resid)/len(rxs_valid)\n",
    "    validation_errors.append(np.array([deg, err_valid]))\n",
    "fit_res_valid = np.stack(validation_errors, axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "filter = ((fit_res_valid[:,0]>0) & (fit_res_valid[:,0]<25) )\n",
    "plt.scatter(fit_res_train[filter,0], fit_res_train[filter,1])\n",
    "plt.scatter(fit_res_valid[filter,0], fit_res_valid[filter,1], c='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We see that this *validation error* behaves at the begining similarly to training error. It's decreases with increasing degree. But at certain moment it start to increase in this case quite dramaticaly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This unfortunately is also typical. Let's look what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(rxs_valid.min(), rxs_valid.max(), 500)\n",
    "plt.scatter(rxs,rys, alpha=0.7, color='none', edgecolor=\"black\", label=\"training\");\n",
    "plt.scatter(rxs_valid,rys_valid, alpha=0.7, color='none', edgecolor=\"red\", label=\"validation\");\n",
    "for i in range(8):\n",
    "    ys = np.polyval(polys[i],xs)\n",
    "    plt.plot(xs,ys);\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.fit(rxs.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "poly_rxs = poly.fit_transform(rxs.reshape(-1,1))\n",
    "t_rxs = torch.from_numpy(poly_rxs.astype('float32'))\n",
    "t_rys = torch.from_numpy(rys.reshape(-1,1).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_rxs_valid = poly.fit_transform(rxs_valid.reshape(-1,1))\n",
    "t_rxs_valid = torch.from_numpy(poly_rxs_valid.astype('float32'))\n",
    "t_rys_valid = torch.from_numpy(rys.reshape(-1,1).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(in_features=t_rxs.shape[-1], out_features=1)\n",
    "torch.nn.init.uniform_(linear.weight,-1.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(linear.parameters(), lr=0.001, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(10000):\n",
    "    optim.zero_grad()\n",
    "    pred = linear(t_rxs)\n",
    "    loss = torch.nn.functional.mse_loss(pred, t_rys)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "print(loss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "torch.nn.functional.mse_loss(linear(t_rxs_valid), t_rys_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge(fit_intercept=False, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = reg.fit(poly_rxs, rys.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.1",
    "jupytext_version": "1.2.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
