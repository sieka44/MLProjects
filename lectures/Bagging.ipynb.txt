{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from torch._utils import _accumulate\n",
    "def random_split(dataset, lengths):\n",
    "    r\"\"\"\n",
    "    Randomly split a dataset into non-overlapping new datasets of given lengths.\n",
    "\n",
    "    Arguments:\n",
    "        dataset (Dataset): Dataset to be split\n",
    "        lengths (sequence): lengths of splits to be produced\n",
    "    \"\"\"\n",
    "    if sum(lengths) != len(dataset):\n",
    "        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n",
    "\n",
    "    indices = torch.randperm(sum(lengths))\n",
    "    return [torch.utils.data.Subset(dataset, indices[offset - length:offset]) for offset, length in zip(_accumulate(lengths), lengths)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## FashionMNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dl_train = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.FashionMNIST('./data/f_mnist', train=True, download=True))\n",
    "\n",
    "dl_test  = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.FashionMNIST('./data/f_mnist', train=False, download=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data   = dl_train.dataset.data.to(dtype=torch.float32)\n",
    "labels = dl_train.dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig_mnist, ax = plt.subplots(1,8, figsize=(8*4,4))\n",
    "for i in range(8):\n",
    "    ax[i].imshow(data[i].numpy(), cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset( \n",
    "    (data/256.0).view(-1,28*28), \n",
    "    labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset = random_split(dataset, (50000,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    torch.utils.data.Subset(train_dataset,torch.randint(len(train_dataset),(len(train_dataset),))) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_loaders = [torch.utils.data.DataLoader(d, \n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True) for d in datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_data   = dl_test.dataset.data.to(dtype=torch.float32)\n",
    "test_labels = dl_test.dataset.targets\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    (test_data/256.0).view(-1,28*28), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "models = [torch.nn.Sequential(\n",
    "    nn.Linear(28*28,1200), nn.ReLU(),\n",
    "    nn.Linear(1200,1200), nn.ReLU(),\n",
    "    nn.Linear(1200,1200), nn.ReLU(),\n",
    "    nn.Linear(1200,10)\n",
    ") for i in range(len(datasets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "optims = [torch.optim.SGD(models[i].parameters(), lr=0.01, momentum=0.6) for i in range(len(datasets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "loss_f = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(datasets)):\n",
    "    for e in range(10):\n",
    "        for d in train_loaders[i]:        \n",
    "            optims[i].zero_grad()\n",
    "            features, labels = d\n",
    "            pred = models[i](features)\n",
    "            loss = loss_f(pred, labels)\n",
    "            loss.backward()\n",
    "            optims[i].step()\n",
    "        print(i,e,loss.item())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(len(datasets)):\n",
    "        pred = torch.softmax(models[i](datasets[i][:][0]),1)\n",
    "        ac = torch.sum(torch.argmax(pred,1)==datasets[i][:][1]).to(dtype=torch.float32)/len(datasets[i])\n",
    "        print(ac)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(len(datasets)):\n",
    "        pred = torch.softmax(models[i](validation_dataset[:][0]),1)\n",
    "        ac = torch.sum(torch.argmax(pred,1)==validation_dataset[:][1]).to(dtype=torch.float32)/len(test_dataset)\n",
    "        print(ac)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def predict_class (model, inp):\n",
    "    pred = torch.softmax(model(inp),dim=1)\n",
    "    return torch.argmax(pred,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [predict_class(models[i], validation_dataset[:][0]) for i in range(len(datasets))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def vote(predictions):\n",
    "    npred=torch.stack(predictions, dim=1).numpy()\n",
    "    v  = np.apply_along_axis(lambda r: np.bincount(r).argmax(), 1, npred)\n",
    "    return torch.from_numpy(v)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gold = vote(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "torch.sum(gold==validation_dataset[:][1]).item()/len(gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<p style=\"text-align:center\"><img style=\"padding:24pt\" src=\"dropout.png\"/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(28*28,1200), nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(1200,1200), nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(1200,1200), nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(1200,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.01 , momentum=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for e in range(10):\n",
    "        for d in train_loader:        \n",
    "            optim.zero_grad()\n",
    "            features, labels = d\n",
    "            pred = model(features)\n",
    "            loss = loss_f(pred, labels)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "        print(e,loss.item())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred = torch.softmax(model(train_dataset[:][0]),1)\n",
    "ac = torch.sum(torch.argmax(pred,1)==train_dataset[:][1]).to(dtype=torch.float32)/len(train_dataset)\n",
    "print(ac)\n",
    "pred = torch.softmax(model(validation_dataset[:][0]),1)\n",
    "ac = torch.sum(torch.argmax(pred,1)==validation_dataset[:][1]).to(dtype=torch.float32)/len(validation_dataset)\n",
    "print(ac)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".Rmd",
    "format_name": "rmarkdown",
    "format_version": "1.1",
    "jupytext_version": "1.2.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
