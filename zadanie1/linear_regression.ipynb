{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j, \\quad x_{i,-1}=1,\\quad b=w_{-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def linear(x,w):\n",
    "    return x @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Generate a random feature vector $\\mathbf{x}$ witch 10000 samples and three feature \n",
    "such that first feature is drawn from N(0,1), second feature from  U(,1) and third from N(1,2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "N(mu,sigma) denotes normal distribution with mean mu and standard deviation sigma. You can use ``numpy.random.normal`` and ``numpy.random.uniform`` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using $\\mathbf{x}$ and weights w = [0.2, 0.5,-0.25,1.0] generate output $\\mathbf{y}$ assuming a $N(0,0.1)$ noise $\\mathbf{\\epsilon}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$ y_i = x_{ij} w_j+\\epsilon_i, \\quad x_{i,-1}=1,\\quad b=w_{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\frac{1}{2}\\frac{1}{N}\\sum_{i=0}^{N-1} (y_i -  x_{ij} w_j  )^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[ 0.2   0.5  -0.25]",
      "\n",
      "[[-0.1542462  -1.71387555 -3.10841518]\n [-1.28812839 -2.02166673 -1.55597569]\n [-0.13611584  0.22465953 -0.43794157]\n [-0.72921205  0.74184753  2.48140871]\n [-1.01964645  0.48577868  0.48729923]\n [ 1.36515182  1.46765055  1.69898819]\n [ 1.9350056  -0.04091108 -1.85659773]\n [-0.90823159  0.47379474  2.33687514]\n [ 0.49212619 -1.17373707 -2.16787315]\n [-0.1819442  -0.69115747 -2.39002991]]",
      "\n",
      "[[-0.0860665 ]\n [-0.98170197]\n [ 0.06027654]\n [-0.44957531]\n [-0.13596702]\n [ 0.53037306]\n [ 0.82036704]\n [-0.42570698]\n [ 0.09189029]\n [ 0.19882579]]",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#Generate data    \n",
    "plt.rcParams['figure.figsize'] = (12.0, 9.0)\n",
    "n = 10000\n",
    "x1 = np.random.normal(0, 1, n)\n",
    "x2 = np.random.normal(0, 1, n)\n",
    "x3 = np.random.normal(1, 2, n)\n",
    "eps = np.random.normal(0.0, 0.1, n)\n",
    "w = np.array([0.2,0.5,-0.25])\n",
    "X = np.transpose(np.array([x1,x2,x3]))\n",
    "print(w)\n",
    "print(X)\n",
    "Y = X.dot(w) + eps\n",
    "Y = Y.reshape(n,1)\n",
    "print(Y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the gradient of the loss function with respect to weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write gradient function ``grad(y,x,w)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "def grad(x,y,w):\n",
    "    return np.array((\n",
    "        np.sum( derivative_x(x,y,w)),\n",
    "        np.sum(derivative_y(x,y,w))\n",
    "    ))/(2*len(x))\n",
    "    \n",
    "def derivative_x(x, y, w):\n",
    "    return -2*y*w+2*x*np.power(w,2)\n",
    "def derivative_y(x, y, w):\n",
    "    return 2*y-2*x*w\n",
    "# def derivative_w(x, y, w):\n",
    "#     return -2*y*w+2*pow(x,2)*w\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement gradient descent for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def gradient_descent(X, y, me, ce, learning_rate=0.000001):\n",
    "    y_pred = me*X + ce  # The current predicted value of Y\n",
    "    d_m = (-2/n) * sum(X * (y - y_pred))  # Derivative wrt m\n",
    "    d_c = (-2/n) * sum(y - y_pred)  # Derivative wrt c\n",
    "    me = me - learning_rate * d_m  # Update m\n",
    "    ce = ce - learning_rate * d_c  # Update c\n",
    "    return me, ce\n",
    "\n",
    "#CORE\n",
    "max_epoch = 500\n",
    "m = 0\n",
    "c = 0\n",
    "\n",
    "for epoch in range(1,max_epoch+1):\n",
    "    m,c = gradient_descent(X, Y, m, c)\n",
    "    if epoch%100 ==0:\n",
    "        print(epoch)\n",
    "        print (m, c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   },
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "100",
      "\n",
      "[ 8.88560636e-05  4.08754259e-05 -3.40750195e-05]",
      " ",
      "[-7.54440382e-06 -7.54404430e-06 -7.54647610e-06]",
      "\n",
      "200",
      "\n",
      "[ 1.77694297e-04  8.17406248e-05 -6.81228738e-05]",
      " ",
      "[-1.50861880e-05 -1.50847429e-05 -1.50945166e-05]",
      "\n",
      "300",
      "\n",
      "[ 0.00026651  0.0001226  -0.00010214]",
      " ",
      "[-2.26253533e-05 -2.26220969e-05 -2.26441189e-05]",
      "\n",
      "400",
      "\n",
      "[ 0.00035532  0.00016344 -0.00013614]",
      " ",
      "[-3.01619004e-05 -3.01561075e-05 -3.01952800e-05]",
      "\n",
      "500",
      "\n",
      "[ 0.0004441   0.00020427 -0.0001701 ]",
      " ",
      "[-3.76958302e-05 -3.76867758e-05 -3.77479973e-05]",
      "\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "y = 0.01 x + -0.00",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "max_epoch = 10000\n",
    "m, c = 0,0\n",
    "\n",
    "for epoch in range(1,max_epoch+1):\n",
    "    sample = np.random.randint(0,n)\n",
    "    x = X[sample]\n",
    "    y= Y[sample]\n",
    "    m,c = gradient_descent(x, y, m, c, 0.00001)\n",
    "    # if epoch % 1000 == 1:\n",
    "    #     print(f'y = {w_s:.2f} x + {b_s:.2f}')\n",
    "        \n",
    "print(f'y = {m:.2f} x + {c:.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement SGD using pytorch. Start by just rewritting Problem 3 to use torch Tensors instead of numpy arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert frrom numpy arrays to torch tensors you can use ``torch.from_numpy()`` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "tensor([0.3076])",
      " ",
      "tensor([0.1237])",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def gradient_descent_tensor(X, y, w, b, learning_rate=0.000001):\n",
    "    dw = -2 * torch.sum(X * (y - w * X - b)) # ∂e/∂w\n",
    "    db = -2 * torch.sum(y - w * X - b)       # ∂e/∂b\n",
    "    w_new = w - learning_rate * dw        # minus sign since we are minizing e\n",
    "    b_new = b - learning_rate * db\n",
    "    return w_new, b_new\n",
    "\n",
    "x_tensor = torch.from_numpy(X).float()\n",
    "y_tensor = torch.from_numpy(Y).float()\n",
    "torch.manual_seed(42)\n",
    "n_epochs = 1000\n",
    "w_t = torch.randn(1, requires_grad=False, dtype=torch.float)\n",
    "b_t = torch.randn(1, requires_grad=False, dtype=torch.float)\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    w_t,b_t = gradient_descent_tensor(x_tensor, y_tensor, w_t, b_t)\n",
    "\n",
    "print(w_t, b_t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%    \n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement GD using pytorch automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this end the variable with respect to which the gradient will be calculated, ``t_w`` in this case, must have attribute\n",
    "``requires_grad`` set to ``True`` (``t_w.require_grad=True``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch will automatically track any expression containing ``t_w`` and store its computational graph. The method ``backward()`` can be run on the final expression to back propagate the gradient e.g. ``loss.backward()``. Then the gradient is accesible as ``t_w.grad``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\Kamil\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:431: UserWarning: Using a target size (torch.Size([10, 3])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "tensor([-0.0196], requires_grad=True)",
      " ",
      "tensor([0.0737], requires_grad=True)",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "n_epochs = 1000\n",
    "learning = 0.01\n",
    "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD([a,b], lr=learning)\n",
    "for epoch in range(1,n_epochs+1):\n",
    "    yhat = a + b * x_tensor\n",
    "    # error = y_tensor - yhat\n",
    "    # loss = (error ** 2).mean() \n",
    "    loss = loss_fn(y_tensor,yhat)\n",
    "    loss.backward()\n",
    "    # with torch.no_grad():\n",
    "    #     a -= lr * a.grad\n",
    "    #     b -= lr * b.grad\n",
    "    optimizer.step()\n",
    "    # a.grad.zero_()\n",
    "    # b.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "print(a,b)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}