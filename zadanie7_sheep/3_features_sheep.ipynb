{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "3-features-sheep.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz-TlX-KSNC9",
        "colab_type": "text"
      },
      "source": [
        "# Interpretability I: Feature Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iZNT7yeSNC_",
        "colab_type": "text"
      },
      "source": [
        "## Copyright notice\n",
        "\n",
        "Parts of this code are adapted from https://pastebin.com/ETXc7Xma and the [Keras example](https://github.com/keras-team/keras/blob/master/examples/conv_filter_visualization.py), (c) 2015 - 2018, Fran√ßois Chollet, [MIT License](https://github.com/keras-team/keras/blob/master/LICENSE). This version (c) 2018 Fabian Offert, [MIT License](LICENSE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebRkt_ODSNDB",
        "colab_type": "text"
      },
      "source": [
        "## Background\n",
        "\n",
        "Feature visualization has been an important area of research within machine learning in general and deep learning in particular at least since 2014 [Zeiler 2014, Simonyan 2014]. \"Deep Dream\", for instance, works by applying feature visualization techniques to images, albeit optimized for producing the kind of visuals it has become famous for. Since then, particularly with the invention of GANs, more elaborate methods have emerged that employ natural image priors to \"bias\" visualizations towards more \"legible\" images [Dosovitskiy 2016, Nguyen 2016a, Nguyen 206b, Nguyen 2017]. Recently, feature visualization and related methods have received a lot of attention as possible solutions to the problem of interpretability, most prominently in [Olah 2017, Olah 2018]. Nevertheless, almost all visualization methods rely on the principle of activation maximization. They visualize the learned features of a particular neuron/channel/layer by optimizing an input image to maximally activate this neuron/channel/layer. \n",
        "\n",
        "Below, we visualize the features of selected channels from the InceptionV1 (also known as GoogLeNet) network, trained on ImageNet. As [Olah 2018] point out, this particular network seems to produce much more legible visualizations then comparable (newer) networks, even without supplying natural image priors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfl0iBcISNDD",
        "colab_type": "text"
      },
      "source": [
        "## Imports\n",
        "\n",
        "We are importing almost the same libraries as in the [\"Deep Dreaming\" notebook](2-deepdream.ipynb), except for two filter functions from SciPy, and two libraries to interface with the operating system. We need these later to run ImageMagick on our images to produce a nice montage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "T_NY7LxGSNDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from keras.applications import VGG16\n",
        "import keras\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.ndimage.filters import gaussian_filter, median_filter\n",
        "from keras import backend as K\n",
        "from io import BytesIO\n",
        "import PIL.Image\n",
        "from IPython.display import clear_output, Image, display\n",
        "from subprocess import call\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBDOiPa7SNDL",
        "colab_type": "text"
      },
      "source": [
        "## Settings\n",
        "\n",
        "We use InceptionV1 as our model. As this architecture does not ship with Keras, we utilize [this custom implementation](https://github.com/fchollet/deep-learning-models/pull/59), with some minor changes/fixes. Most importantly, we change the softmax activation function into a linear activation function, as suggested in [Simonyan 2014]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-skqiiQSNDM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e4666dc3-1754-4ae6-fe75-f6921d6c71f9"
      },
      "source": [
        "# We are in the \"test\" phase, not the \"training\" phase, w.r.t. to the model we are analyzing\n",
        "K.set_learning_phase(0)\n",
        "\n",
        "# Load model with ImageNet pre-trained weights\n",
        "# Source: https://github.com/fchollet/deep-learning-models/pull/59\n",
        "# import inception_v1_linear\n",
        "model = keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None, input_shape=None, pooling=None, classes=1000)\n",
        "# model = inception_v1_linear.InceptionV1(weights='imagenet', include_top=True) \n",
        "# model = VGG16(weights='imagenet',\n",
        "#                   include_top=False)\n",
        "model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"inception_v3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 299, 299, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_283 (Conv2D)             (None, 149, 149, 32) 864         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_283 (BatchN (None, 149, 149, 32) 96          conv2d_283[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_283 (Activation)     (None, 149, 149, 32) 0           batch_normalization_283[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_284 (Conv2D)             (None, 147, 147, 32) 9216        activation_283[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_284 (BatchN (None, 147, 147, 32) 96          conv2d_284[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_284 (Activation)     (None, 147, 147, 32) 0           batch_normalization_284[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_285 (Conv2D)             (None, 147, 147, 64) 18432       activation_284[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_285 (BatchN (None, 147, 147, 64) 192         conv2d_285[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_285 (Activation)     (None, 147, 147, 64) 0           batch_normalization_285[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling2D) (None, 73, 73, 64)   0           activation_285[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_286 (Conv2D)             (None, 73, 73, 80)   5120        max_pooling2d_13[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_286 (BatchN (None, 73, 73, 80)   240         conv2d_286[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_286 (Activation)     (None, 73, 73, 80)   0           batch_normalization_286[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_287 (Conv2D)             (None, 71, 71, 192)  138240      activation_286[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_287 (BatchN (None, 71, 71, 192)  576         conv2d_287[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_287 (Activation)     (None, 71, 71, 192)  0           batch_normalization_287[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling2D) (None, 35, 35, 192)  0           activation_287[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_291 (Conv2D)             (None, 35, 35, 64)   12288       max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_291 (BatchN (None, 35, 35, 64)   192         conv2d_291[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_291 (Activation)     (None, 35, 35, 64)   0           batch_normalization_291[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_289 (Conv2D)             (None, 35, 35, 48)   9216        max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_292 (Conv2D)             (None, 35, 35, 96)   55296       activation_291[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_289 (BatchN (None, 35, 35, 48)   144         conv2d_289[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_292 (BatchN (None, 35, 35, 96)   288         conv2d_292[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_289 (Activation)     (None, 35, 35, 48)   0           batch_normalization_289[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_292 (Activation)     (None, 35, 35, 96)   0           batch_normalization_292[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_28 (AveragePo (None, 35, 35, 192)  0           max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_288 (Conv2D)             (None, 35, 35, 64)   12288       max_pooling2d_14[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_290 (Conv2D)             (None, 35, 35, 64)   76800       activation_289[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_293 (Conv2D)             (None, 35, 35, 96)   82944       activation_292[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_294 (Conv2D)             (None, 35, 35, 32)   6144        average_pooling2d_28[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_288 (BatchN (None, 35, 35, 64)   192         conv2d_288[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_290 (BatchN (None, 35, 35, 64)   192         conv2d_290[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_293 (BatchN (None, 35, 35, 96)   288         conv2d_293[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_294 (BatchN (None, 35, 35, 32)   96          conv2d_294[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_288 (Activation)     (None, 35, 35, 64)   0           batch_normalization_288[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_290 (Activation)     (None, 35, 35, 64)   0           batch_normalization_290[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_293 (Activation)     (None, 35, 35, 96)   0           batch_normalization_293[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_294 (Activation)     (None, 35, 35, 32)   0           batch_normalization_294[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_288[0][0]             \n",
            "                                                                 activation_290[0][0]             \n",
            "                                                                 activation_293[0][0]             \n",
            "                                                                 activation_294[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_298 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_298 (BatchN (None, 35, 35, 64)   192         conv2d_298[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_298 (Activation)     (None, 35, 35, 64)   0           batch_normalization_298[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_296 (Conv2D)             (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_299 (Conv2D)             (None, 35, 35, 96)   55296       activation_298[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_296 (BatchN (None, 35, 35, 48)   144         conv2d_296[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_299 (BatchN (None, 35, 35, 96)   288         conv2d_299[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_296 (Activation)     (None, 35, 35, 48)   0           batch_normalization_296[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_299 (Activation)     (None, 35, 35, 96)   0           batch_normalization_299[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_29 (AveragePo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_295 (Conv2D)             (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_297 (Conv2D)             (None, 35, 35, 64)   76800       activation_296[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_300 (Conv2D)             (None, 35, 35, 96)   82944       activation_299[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_301 (Conv2D)             (None, 35, 35, 64)   16384       average_pooling2d_29[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_295 (BatchN (None, 35, 35, 64)   192         conv2d_295[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_297 (BatchN (None, 35, 35, 64)   192         conv2d_297[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_300 (BatchN (None, 35, 35, 96)   288         conv2d_300[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_301 (BatchN (None, 35, 35, 64)   192         conv2d_301[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_295 (Activation)     (None, 35, 35, 64)   0           batch_normalization_295[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_297 (Activation)     (None, 35, 35, 64)   0           batch_normalization_297[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_300 (Activation)     (None, 35, 35, 96)   0           batch_normalization_300[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_301 (Activation)     (None, 35, 35, 64)   0           batch_normalization_301[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_295[0][0]             \n",
            "                                                                 activation_297[0][0]             \n",
            "                                                                 activation_300[0][0]             \n",
            "                                                                 activation_301[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_305 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_305 (BatchN (None, 35, 35, 64)   192         conv2d_305[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_305 (Activation)     (None, 35, 35, 64)   0           batch_normalization_305[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_303 (Conv2D)             (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_306 (Conv2D)             (None, 35, 35, 96)   55296       activation_305[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_303 (BatchN (None, 35, 35, 48)   144         conv2d_303[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_306 (BatchN (None, 35, 35, 96)   288         conv2d_306[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_303 (Activation)     (None, 35, 35, 48)   0           batch_normalization_303[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_306 (Activation)     (None, 35, 35, 96)   0           batch_normalization_306[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_30 (AveragePo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_302 (Conv2D)             (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_304 (Conv2D)             (None, 35, 35, 64)   76800       activation_303[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_307 (Conv2D)             (None, 35, 35, 96)   82944       activation_306[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_308 (Conv2D)             (None, 35, 35, 64)   18432       average_pooling2d_30[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_302 (BatchN (None, 35, 35, 64)   192         conv2d_302[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_304 (BatchN (None, 35, 35, 64)   192         conv2d_304[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_307 (BatchN (None, 35, 35, 96)   288         conv2d_307[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_308 (BatchN (None, 35, 35, 64)   192         conv2d_308[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_302 (Activation)     (None, 35, 35, 64)   0           batch_normalization_302[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_304 (Activation)     (None, 35, 35, 64)   0           batch_normalization_304[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_307 (Activation)     (None, 35, 35, 96)   0           batch_normalization_307[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_308 (Activation)     (None, 35, 35, 64)   0           batch_normalization_308[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_302[0][0]             \n",
            "                                                                 activation_304[0][0]             \n",
            "                                                                 activation_307[0][0]             \n",
            "                                                                 activation_308[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_310 (Conv2D)             (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_310 (BatchN (None, 35, 35, 64)   192         conv2d_310[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_310 (Activation)     (None, 35, 35, 64)   0           batch_normalization_310[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_311 (Conv2D)             (None, 35, 35, 96)   55296       activation_310[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_311 (BatchN (None, 35, 35, 96)   288         conv2d_311[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_311 (Activation)     (None, 35, 35, 96)   0           batch_normalization_311[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_309 (Conv2D)             (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_312 (Conv2D)             (None, 17, 17, 96)   82944       activation_311[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_309 (BatchN (None, 17, 17, 384)  1152        conv2d_309[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_312 (BatchN (None, 17, 17, 96)   288         conv2d_312[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_309 (Activation)     (None, 17, 17, 384)  0           batch_normalization_309[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_312 (Activation)     (None, 17, 17, 96)   0           batch_normalization_312[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_15 (MaxPooling2D) (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_309[0][0]             \n",
            "                                                                 activation_312[0][0]             \n",
            "                                                                 max_pooling2d_15[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_317 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_317 (BatchN (None, 17, 17, 128)  384         conv2d_317[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_317 (Activation)     (None, 17, 17, 128)  0           batch_normalization_317[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_318 (Conv2D)             (None, 17, 17, 128)  114688      activation_317[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_318 (BatchN (None, 17, 17, 128)  384         conv2d_318[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_318 (Activation)     (None, 17, 17, 128)  0           batch_normalization_318[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_314 (Conv2D)             (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_319 (Conv2D)             (None, 17, 17, 128)  114688      activation_318[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_314 (BatchN (None, 17, 17, 128)  384         conv2d_314[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_319 (BatchN (None, 17, 17, 128)  384         conv2d_319[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_314 (Activation)     (None, 17, 17, 128)  0           batch_normalization_314[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_319 (Activation)     (None, 17, 17, 128)  0           batch_normalization_319[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_315 (Conv2D)             (None, 17, 17, 128)  114688      activation_314[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_320 (Conv2D)             (None, 17, 17, 128)  114688      activation_319[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_315 (BatchN (None, 17, 17, 128)  384         conv2d_315[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_320 (BatchN (None, 17, 17, 128)  384         conv2d_320[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_315 (Activation)     (None, 17, 17, 128)  0           batch_normalization_315[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_320 (Activation)     (None, 17, 17, 128)  0           batch_normalization_320[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_31 (AveragePo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_313 (Conv2D)             (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_316 (Conv2D)             (None, 17, 17, 192)  172032      activation_315[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_321 (Conv2D)             (None, 17, 17, 192)  172032      activation_320[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_322 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_31[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_313 (BatchN (None, 17, 17, 192)  576         conv2d_313[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_316 (BatchN (None, 17, 17, 192)  576         conv2d_316[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_321 (BatchN (None, 17, 17, 192)  576         conv2d_321[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_322 (BatchN (None, 17, 17, 192)  576         conv2d_322[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_313 (Activation)     (None, 17, 17, 192)  0           batch_normalization_313[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_316 (Activation)     (None, 17, 17, 192)  0           batch_normalization_316[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_321 (Activation)     (None, 17, 17, 192)  0           batch_normalization_321[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_322 (Activation)     (None, 17, 17, 192)  0           batch_normalization_322[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_313[0][0]             \n",
            "                                                                 activation_316[0][0]             \n",
            "                                                                 activation_321[0][0]             \n",
            "                                                                 activation_322[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_327 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_327 (BatchN (None, 17, 17, 160)  480         conv2d_327[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_327 (Activation)     (None, 17, 17, 160)  0           batch_normalization_327[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_328 (Conv2D)             (None, 17, 17, 160)  179200      activation_327[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_328 (BatchN (None, 17, 17, 160)  480         conv2d_328[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_328 (Activation)     (None, 17, 17, 160)  0           batch_normalization_328[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_324 (Conv2D)             (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_329 (Conv2D)             (None, 17, 17, 160)  179200      activation_328[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_324 (BatchN (None, 17, 17, 160)  480         conv2d_324[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_329 (BatchN (None, 17, 17, 160)  480         conv2d_329[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_324 (Activation)     (None, 17, 17, 160)  0           batch_normalization_324[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_329 (Activation)     (None, 17, 17, 160)  0           batch_normalization_329[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_325 (Conv2D)             (None, 17, 17, 160)  179200      activation_324[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_330 (Conv2D)             (None, 17, 17, 160)  179200      activation_329[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_325 (BatchN (None, 17, 17, 160)  480         conv2d_325[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_330 (BatchN (None, 17, 17, 160)  480         conv2d_330[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_325 (Activation)     (None, 17, 17, 160)  0           batch_normalization_325[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_330 (Activation)     (None, 17, 17, 160)  0           batch_normalization_330[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_32 (AveragePo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_323 (Conv2D)             (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_326 (Conv2D)             (None, 17, 17, 192)  215040      activation_325[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_331 (Conv2D)             (None, 17, 17, 192)  215040      activation_330[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_332 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_32[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_323 (BatchN (None, 17, 17, 192)  576         conv2d_323[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_326 (BatchN (None, 17, 17, 192)  576         conv2d_326[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_331 (BatchN (None, 17, 17, 192)  576         conv2d_331[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_332 (BatchN (None, 17, 17, 192)  576         conv2d_332[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_323 (Activation)     (None, 17, 17, 192)  0           batch_normalization_323[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_326 (Activation)     (None, 17, 17, 192)  0           batch_normalization_326[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_331 (Activation)     (None, 17, 17, 192)  0           batch_normalization_331[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_332 (Activation)     (None, 17, 17, 192)  0           batch_normalization_332[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_323[0][0]             \n",
            "                                                                 activation_326[0][0]             \n",
            "                                                                 activation_331[0][0]             \n",
            "                                                                 activation_332[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_337 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_337 (BatchN (None, 17, 17, 160)  480         conv2d_337[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_337 (Activation)     (None, 17, 17, 160)  0           batch_normalization_337[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_338 (Conv2D)             (None, 17, 17, 160)  179200      activation_337[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_338 (BatchN (None, 17, 17, 160)  480         conv2d_338[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_338 (Activation)     (None, 17, 17, 160)  0           batch_normalization_338[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_334 (Conv2D)             (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_339 (Conv2D)             (None, 17, 17, 160)  179200      activation_338[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_334 (BatchN (None, 17, 17, 160)  480         conv2d_334[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_339 (BatchN (None, 17, 17, 160)  480         conv2d_339[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_334 (Activation)     (None, 17, 17, 160)  0           batch_normalization_334[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_339 (Activation)     (None, 17, 17, 160)  0           batch_normalization_339[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_335 (Conv2D)             (None, 17, 17, 160)  179200      activation_334[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_340 (Conv2D)             (None, 17, 17, 160)  179200      activation_339[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_335 (BatchN (None, 17, 17, 160)  480         conv2d_335[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_340 (BatchN (None, 17, 17, 160)  480         conv2d_340[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_335 (Activation)     (None, 17, 17, 160)  0           batch_normalization_335[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_340 (Activation)     (None, 17, 17, 160)  0           batch_normalization_340[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_33 (AveragePo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_333 (Conv2D)             (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_336 (Conv2D)             (None, 17, 17, 192)  215040      activation_335[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_341 (Conv2D)             (None, 17, 17, 192)  215040      activation_340[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_342 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_33[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_333 (BatchN (None, 17, 17, 192)  576         conv2d_333[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_336 (BatchN (None, 17, 17, 192)  576         conv2d_336[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_341 (BatchN (None, 17, 17, 192)  576         conv2d_341[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_342 (BatchN (None, 17, 17, 192)  576         conv2d_342[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_333 (Activation)     (None, 17, 17, 192)  0           batch_normalization_333[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_336 (Activation)     (None, 17, 17, 192)  0           batch_normalization_336[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_341 (Activation)     (None, 17, 17, 192)  0           batch_normalization_341[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_342 (Activation)     (None, 17, 17, 192)  0           batch_normalization_342[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_333[0][0]             \n",
            "                                                                 activation_336[0][0]             \n",
            "                                                                 activation_341[0][0]             \n",
            "                                                                 activation_342[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_347 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_347 (BatchN (None, 17, 17, 192)  576         conv2d_347[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_347 (Activation)     (None, 17, 17, 192)  0           batch_normalization_347[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_348 (Conv2D)             (None, 17, 17, 192)  258048      activation_347[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_348 (BatchN (None, 17, 17, 192)  576         conv2d_348[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_348 (Activation)     (None, 17, 17, 192)  0           batch_normalization_348[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_344 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_349 (Conv2D)             (None, 17, 17, 192)  258048      activation_348[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_344 (BatchN (None, 17, 17, 192)  576         conv2d_344[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_349 (BatchN (None, 17, 17, 192)  576         conv2d_349[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_344 (Activation)     (None, 17, 17, 192)  0           batch_normalization_344[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_349 (Activation)     (None, 17, 17, 192)  0           batch_normalization_349[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_345 (Conv2D)             (None, 17, 17, 192)  258048      activation_344[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_350 (Conv2D)             (None, 17, 17, 192)  258048      activation_349[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_345 (BatchN (None, 17, 17, 192)  576         conv2d_345[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_350 (BatchN (None, 17, 17, 192)  576         conv2d_350[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_345 (Activation)     (None, 17, 17, 192)  0           batch_normalization_345[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_350 (Activation)     (None, 17, 17, 192)  0           batch_normalization_350[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_34 (AveragePo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_343 (Conv2D)             (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_346 (Conv2D)             (None, 17, 17, 192)  258048      activation_345[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_351 (Conv2D)             (None, 17, 17, 192)  258048      activation_350[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_352 (Conv2D)             (None, 17, 17, 192)  147456      average_pooling2d_34[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_343 (BatchN (None, 17, 17, 192)  576         conv2d_343[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_346 (BatchN (None, 17, 17, 192)  576         conv2d_346[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_351 (BatchN (None, 17, 17, 192)  576         conv2d_351[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_352 (BatchN (None, 17, 17, 192)  576         conv2d_352[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_343 (Activation)     (None, 17, 17, 192)  0           batch_normalization_343[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_346 (Activation)     (None, 17, 17, 192)  0           batch_normalization_346[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_351 (Activation)     (None, 17, 17, 192)  0           batch_normalization_351[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_352 (Activation)     (None, 17, 17, 192)  0           batch_normalization_352[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_343[0][0]             \n",
            "                                                                 activation_346[0][0]             \n",
            "                                                                 activation_351[0][0]             \n",
            "                                                                 activation_352[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_355 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_355 (BatchN (None, 17, 17, 192)  576         conv2d_355[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_355 (Activation)     (None, 17, 17, 192)  0           batch_normalization_355[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_356 (Conv2D)             (None, 17, 17, 192)  258048      activation_355[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_356 (BatchN (None, 17, 17, 192)  576         conv2d_356[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_356 (Activation)     (None, 17, 17, 192)  0           batch_normalization_356[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_353 (Conv2D)             (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_357 (Conv2D)             (None, 17, 17, 192)  258048      activation_356[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_353 (BatchN (None, 17, 17, 192)  576         conv2d_353[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_357 (BatchN (None, 17, 17, 192)  576         conv2d_357[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_353 (Activation)     (None, 17, 17, 192)  0           batch_normalization_353[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_357 (Activation)     (None, 17, 17, 192)  0           batch_normalization_357[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_354 (Conv2D)             (None, 8, 8, 320)    552960      activation_353[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_358 (Conv2D)             (None, 8, 8, 192)    331776      activation_357[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_354 (BatchN (None, 8, 8, 320)    960         conv2d_354[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_358 (BatchN (None, 8, 8, 192)    576         conv2d_358[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_354 (Activation)     (None, 8, 8, 320)    0           batch_normalization_354[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_358 (Activation)     (None, 8, 8, 192)    0           batch_normalization_358[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling2D) (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_354[0][0]             \n",
            "                                                                 activation_358[0][0]             \n",
            "                                                                 max_pooling2d_16[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_363 (Conv2D)             (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_363 (BatchN (None, 8, 8, 448)    1344        conv2d_363[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_363 (Activation)     (None, 8, 8, 448)    0           batch_normalization_363[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_360 (Conv2D)             (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_364 (Conv2D)             (None, 8, 8, 384)    1548288     activation_363[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_360 (BatchN (None, 8, 8, 384)    1152        conv2d_360[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_364 (BatchN (None, 8, 8, 384)    1152        conv2d_364[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_360 (Activation)     (None, 8, 8, 384)    0           batch_normalization_360[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_364 (Activation)     (None, 8, 8, 384)    0           batch_normalization_364[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_361 (Conv2D)             (None, 8, 8, 384)    442368      activation_360[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_362 (Conv2D)             (None, 8, 8, 384)    442368      activation_360[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_365 (Conv2D)             (None, 8, 8, 384)    442368      activation_364[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_366 (Conv2D)             (None, 8, 8, 384)    442368      activation_364[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_35 (AveragePo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_359 (Conv2D)             (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_361 (BatchN (None, 8, 8, 384)    1152        conv2d_361[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_362 (BatchN (None, 8, 8, 384)    1152        conv2d_362[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_365 (BatchN (None, 8, 8, 384)    1152        conv2d_365[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_366 (BatchN (None, 8, 8, 384)    1152        conv2d_366[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_367 (Conv2D)             (None, 8, 8, 192)    245760      average_pooling2d_35[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_359 (BatchN (None, 8, 8, 320)    960         conv2d_359[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_361 (Activation)     (None, 8, 8, 384)    0           batch_normalization_361[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_362 (Activation)     (None, 8, 8, 384)    0           batch_normalization_362[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_365 (Activation)     (None, 8, 8, 384)    0           batch_normalization_365[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_366 (Activation)     (None, 8, 8, 384)    0           batch_normalization_366[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_367 (BatchN (None, 8, 8, 192)    576         conv2d_367[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_359 (Activation)     (None, 8, 8, 320)    0           batch_normalization_359[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_361[0][0]             \n",
            "                                                                 activation_362[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 8, 8, 768)    0           activation_365[0][0]             \n",
            "                                                                 activation_366[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_367 (Activation)     (None, 8, 8, 192)    0           batch_normalization_367[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_359[0][0]             \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_7[0][0]              \n",
            "                                                                 activation_367[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_372 (Conv2D)             (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_372 (BatchN (None, 8, 8, 448)    1344        conv2d_372[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_372 (Activation)     (None, 8, 8, 448)    0           batch_normalization_372[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_369 (Conv2D)             (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_373 (Conv2D)             (None, 8, 8, 384)    1548288     activation_372[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_369 (BatchN (None, 8, 8, 384)    1152        conv2d_369[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_373 (BatchN (None, 8, 8, 384)    1152        conv2d_373[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_369 (Activation)     (None, 8, 8, 384)    0           batch_normalization_369[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_373 (Activation)     (None, 8, 8, 384)    0           batch_normalization_373[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_370 (Conv2D)             (None, 8, 8, 384)    442368      activation_369[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_371 (Conv2D)             (None, 8, 8, 384)    442368      activation_369[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_374 (Conv2D)             (None, 8, 8, 384)    442368      activation_373[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_375 (Conv2D)             (None, 8, 8, 384)    442368      activation_373[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_36 (AveragePo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_368 (Conv2D)             (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_370 (BatchN (None, 8, 8, 384)    1152        conv2d_370[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_371 (BatchN (None, 8, 8, 384)    1152        conv2d_371[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_374 (BatchN (None, 8, 8, 384)    1152        conv2d_374[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_375 (BatchN (None, 8, 8, 384)    1152        conv2d_375[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_376 (Conv2D)             (None, 8, 8, 192)    393216      average_pooling2d_36[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_368 (BatchN (None, 8, 8, 320)    960         conv2d_368[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_370 (Activation)     (None, 8, 8, 384)    0           batch_normalization_370[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_371 (Activation)     (None, 8, 8, 384)    0           batch_normalization_371[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_374 (Activation)     (None, 8, 8, 384)    0           batch_normalization_374[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "activation_375 (Activation)     (None, 8, 8, 384)    0           batch_normalization_375[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_376 (BatchN (None, 8, 8, 192)    576         conv2d_376[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_368 (Activation)     (None, 8, 8, 320)    0           batch_normalization_368[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_370[0][0]             \n",
            "                                                                 activation_371[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 8, 8, 768)    0           activation_374[0][0]             \n",
            "                                                                 activation_375[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_376 (Activation)     (None, 8, 8, 192)    0           batch_normalization_376[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_368[0][0]             \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_8[0][0]              \n",
            "                                                                 activation_376[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 23,851,784\n",
            "Trainable params: 23,817,352\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDGQfE56SNDR",
        "colab_type": "text"
      },
      "source": [
        "We also define two sets of settings: one for visualizing the classes of the model (`settings_InceptionV1_classes`), and one for visualizing arbitrary layers of the model (`settings_InceptionV1_single`). Other than with V3, for InceptionV1 the input size is fixed to $224^2$ pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nK9oikDSNDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "settings_InceptionV1_classes = ['predictions']\n",
        "settings_InceptionV1_single = ['Mixed_4b_Concatenated']\n",
        "settings = settings_InceptionV1_classes\n",
        "size = 224 # 224 for InceptionV1, variable for InceptionV3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CY-qqH-2SNDX",
        "colab_type": "text"
      },
      "source": [
        "Finally, we define which part of the layer we would like to visualize: just one particular channel (e.g. `filters = [8]` would look at the \"hen\" class in the `Predictions` layer), all available channels (`filters = None; sum_filters = False`), or the sum of all available channels (`filters = None; sum_filters = True`). If we are analyzing the output layer, the classes are defined according to [this list](https://github.com/happynear/caffe-windows/blob/master/examples/GoogLeNet/synset_words.txt)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ06w7GqSNDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filters = [351] # If None use all available filters (don't use None for prediction layers, define range!)\n",
        "sum_filters = False # If true, sum all filters, if false iterate over all filters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syKYlpVoSNDa",
        "colab_type": "text"
      },
      "source": [
        "## Image preprocessing and deprocessing\n",
        "\n",
        "We are using the same image helper functions as in the [\"Deep Dreaming\" notebook](3-deepdream.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NdGQni2SNDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path)\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0) # Add extra dimension for mini batches (not used)\n",
        "    img = inception_v1_linear.preprocess_input(img) # 3D -> 1D\n",
        "    return img\n",
        "\n",
        "def deprocess_image(x):\n",
        "    # Check ~/.keras/keras.json to make sure \"image_data_format\" is set to \"channels_last\"\n",
        "    # or print(K.image_data_format())\n",
        "    x = x.reshape((x.shape[1], x.shape[2], 3)) # \"Remove\" extra dimension, channels last\n",
        "    x /= 2.\n",
        "    x += 0.5\n",
        "    x *= 255.\n",
        "    x = np.clip(x, 0, 255).astype('uint8') # Clip to visible range\n",
        "    x = autotone(x)\n",
        "    return x\n",
        "\n",
        "# Simple resize function based on scipy\n",
        "def resize_img(img, size):\n",
        "    img = np.copy(img)\n",
        "    factors = (1, float(size[0]) / img.shape[1], float(size[1]) / img.shape[2], 1)\n",
        "    return scipy.ndimage.zoom(img, factors, order=1)\n",
        "import imageio\n",
        "# Simple save function based on scipy\n",
        "def save_image(img, fname):\n",
        "    pil_img = deprocess_image(np.copy(img))\n",
        "    imageio.imwrite(fname, pil_img)\n",
        "    \n",
        "def show_image(img, fmt='jpeg'):\n",
        "    img = deprocess_image(np.copy(img))\n",
        "    f = BytesIO()\n",
        "    PIL.Image.fromarray(img).save(f, fmt)\n",
        "    display(Image(data=f.getvalue()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ_tL4l9SNDc",
        "colab_type": "text"
      },
      "source": [
        "## More image preprocessing and deprocessing \n",
        "\n",
        "In addition, we define two functions to save images sequences. The first one (`save_image_numbered`) simly creates numbered sequences, the second one (`save_image_sweep`) includes a dictionary into the filename. We use this second function ofr hyperparameter sweeps (see below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPFvC-_eSNDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def montage_images(folder, size):\n",
        "    print('Montaging...')\n",
        "    geometry = '-geometry \"' + str(size) + 'x' + str(size) + '+2+2>\" '\n",
        "    label = '-label \"%t\" '\n",
        "    output = folder + '/montage.jpg'\n",
        "    infiles = folder + '/*.jpg '\n",
        "    montage = 'montage  ' + label + infiles + geometry + output\n",
        "    call(montage, shell=True)\n",
        "    print('...done.')\n",
        "\n",
        "def save_image_numbered(img, nr, folder):\n",
        "    f = '{0:03d}'.format(nr)\n",
        "    p = folder + '/' + f + '.jpg'\n",
        "    save_image(img, p)\n",
        "    \n",
        "def save_image_sweep(img, filter, sweep, folder):\n",
        "    # Concatenate the list of values in the dictionary as strings\n",
        "    f = str(filter) + '_' + '_'.join(str(x) for x in list(sweep.values()))\n",
        "    p = folder + '/' + f + '.jpg'\n",
        "    print('Writing ' + p)\n",
        "    save_image(img, p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42593HUaSNDf",
        "colab_type": "text"
      },
      "source": [
        "## Regularization\n",
        "\n",
        "Regularization introduces priors into the loss function. By utilizing regularization, we end up with \"better\", more legible images. To start, we define a simple \"auto tone\" function (`autotone`) that normalizes each color channel in an image separately ‚Äì exactly what Photoshop is doing in its \"auto tone\" function ‚Äì to get more legible images. We also do not start with a plain gray image but with a gray image that includes some Gaussian white noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rChKyIEsSNDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize each color sepearately (Photoshop auto tone)\n",
        "def autotone(img):\n",
        "    img[:,:,0] = np.interp(img[:,:,0], [np.amin(img[:,:,0]), np.amax(img[:,:,0])], [0, 255])\n",
        "    img[:,:,1] = np.interp(img[:,:,1], [np.amin(img[:,:,1]), np.amax(img[:,:,1])], [0, 255])\n",
        "    img[:,:,2] = np.interp(img[:,:,2], [np.amin(img[:,:,2]), np.amax(img[:,:,2])], [0, 255])\n",
        "    return img\n",
        "\n",
        "def gray_square(size, variance): \n",
        "    img = np.random.normal(0, variance, (1, size, size, 3))\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqWZlllxSNDi",
        "colab_type": "text"
      },
      "source": [
        "Three other regularization functions are embedded into the gradient ascent function: a Gaussian blur function, and a median filter function. Their utilization is controlled by several hyperparameters that define how often these filters are applied (every four iterations, for instance) and how strong they are. By tuning these hyperparameters, either manually or automatically by means of hyperparameter sweeps (see below), we can find settings that subjectively produce better images, images that are more obviously representations of existing concepts. As so often, however, good is better then best, as the \"best\" images, i.e. the images that activate the layer/filter we are looking at the most, are usually just high-frequency noise. As pointed out by [Szegedy 2013], this link between adversarial examples and semantic is one of the most \"intruiging properties\" of neural networks that has [many interesting epistemological implications](https://arxiv.org/abs/1711.08042)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDna2NOZSNDi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_ascent(x, \n",
        "                    iterations=1000, \n",
        "                    step=0.01, \n",
        "                    max_loss=None, \n",
        "                    blur_std=0, \n",
        "                    blur_every=8, \n",
        "                    median_fsize=5, \n",
        "                    median_every=4):\n",
        "    \n",
        "    for i in range(iterations):\n",
        "        outs = fetch_loss_and_grads([x])\n",
        "        loss_value = outs[0]\n",
        "        grad_values = outs[1]\n",
        "        if max_loss is not None and loss_value > max_loss:\n",
        "            break\n",
        "        x += step * grad_values      \n",
        "        \n",
        "        if(i!=iterations-1): # No regularization on last iteration for good quality output \n",
        "            # Gaussian blur\n",
        "            if blur_std is not 0 and i % blur_every == 0 :\n",
        "                x = gaussian_filter(x, sigma=[0, blur_std, blur_std, 0])  \n",
        "            # Median filter\n",
        "            if median_fsize is not 0 and i % median_every == 0 :\n",
        "                x = median_filter(x, size=(1, median_fsize, median_fsize, 1))\n",
        "                \n",
        "    return x\n",
        "\n",
        "# Dictionary of the names of the layers and the layers themselves\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "\n",
        "# Input of the first layer, in the example: model.input\n",
        "dream = model.layers[0].input \n",
        "\n",
        "# A TF variable (persistent)\n",
        "loss = K.variable(0.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDOFzcfJSNDl",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter Sweeping\n",
        "\n",
        "To find the \"right\" hyperparameters for the regularization techniques that we have introduced, we create a \"sweep\" function that conveniently applies sets of pre-defined parameters in all possible permutations to the same class/filter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAfGe7mHSNDm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "919f1078-6afc-4ab5-f8c2-f6bf431eba9b"
      },
      "source": [
        "# Best values InceptionV1 from sweep above\n",
        "iterations = [1000]\n",
        "steps = [0.01]\n",
        "max_losses = [None]\n",
        "blur_stds = [0]\n",
        "blur_everys = [4]\n",
        "median_fsizes = [5]\n",
        "median_everys = [4]\n",
        "variances = [0.01]\n",
        "\n",
        "sweeps = []\n",
        "\n",
        "for iteration in iterations:\n",
        "    for step in steps:\n",
        "        for max_loss in max_losses:\n",
        "            for blur_std in blur_stds:\n",
        "                for blur_every in blur_everys:\n",
        "                    for median_fsize in median_fsizes:\n",
        "                        for median_every in median_everys:\n",
        "                            for variance in variances:\n",
        "                                sweeps.append({'iterations':iteration,\n",
        "                                                'step':step,\n",
        "                                                'max_loss':max_loss,\n",
        "                                                'blur_std':blur_std,\n",
        "                                                'blur_every':blur_every,\n",
        "                                                'median_fsize':median_fsize,\n",
        "                                                'median_every':median_every,\n",
        "                                                'variance':variance})\n",
        "                                \n",
        "print(str(len(sweeps)) + ' sweeps generated')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 sweeps generated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP6Y8E8rSNDo",
        "colab_type": "text"
      },
      "source": [
        "# Activation maximiation\n",
        "\n",
        "We now run the activation maximization process for the selected layers/channels. The process is exactly the same as in the [\"Deep Dreaming\" notebook](3-deepdream.ipynb), except for a simpler loss function if we are looking at classes, as the last layer already gives us a single scalar loss value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMsg_eFbSNDo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "86abe11a-f7ff-4133-f69d-7cf718bf6c96"
      },
      "source": [
        "folder = '3-features/'\n",
        "\n",
        "# Iterate only over the layers picked above and their available filters\n",
        "for layer_name in settings: \n",
        "    assert layer_name in layer_dict.keys(), 'Layer ' + layer_name + ' not found in model.' # Layer in model?\n",
        "    \n",
        "    # Create directory to hold frames\n",
        "    if not os.path.exists(folder + layer_name):\n",
        "        os.makedirs(folder + layer_name)\n",
        "    \n",
        "    x = layer_dict[layer_name].output # Output of the current layer\n",
        "        \n",
        "    if (filters == None and not sum_filters): channels = list(range(x.shape[3])) # Iterate over all filters\n",
        "    elif (filters == None and sum_filters): channels = [1] # Sum all filters\n",
        "    else: channels = filters\n",
        "    \n",
        "    # We might want to stop early and not be stuck with the same filters every time\n",
        "    np.random.shuffle(channels)\n",
        "    \n",
        "    for channel in channels:\n",
        "        \n",
        "        for sweep in sweeps:\n",
        "    \n",
        "            # We avoid border artifacts by only involving non-border pixels in the loss, offset by 2 on all sides\n",
        "            if (filters == None and sum_filters): loss = K.sum(K.mean(x[:, 2: -2, 2: -2, :]))\n",
        "            elif (filters == None and not sum_filters): loss = K.sum(K.mean(x[:, 2: -2, 2: -2, channel]))\n",
        "            # Classification layers just give a single probability, so no sum/mean/offset\n",
        "            else: loss = model.layers[-1].output[0, channel] # Always output of the last layer\n",
        "    \n",
        "            # Compute the gradients of the dream w.r.t. the loss.\n",
        "            grads = K.gradients(loss, dream)[0]\n",
        "            # Normalize gradients.\n",
        "            grads /= K.maximum(K.mean(K.abs(grads)), K.epsilon())\n",
        "\n",
        "            fetch_loss_and_grads = K.function([dream], [loss, grads])\n",
        "\n",
        "            img = gray_square(size, sweep['variance'])\n",
        "            img = gradient_ascent(img, \n",
        "                                  iterations=sweep['iterations'], \n",
        "                                  step=sweep['step'], \n",
        "                                  max_loss=sweep['max_loss'],\n",
        "                                  blur_std=sweep['blur_std'], \n",
        "                                  blur_every=sweep['blur_every'], \n",
        "                                  median_fsize=sweep['median_fsize'], \n",
        "                                  median_every=sweep['median_every'])\n",
        "            if (len(sweeps) > 1): save_image_sweep(img, channel, sweep, '4-features/' + layer_name)\n",
        "            else:\n",
        "                show_image(img)\n",
        "                save_image_numbered(img, channel, folder + layer_name)\n",
        "        \n",
        "    montage_images(layer_name, size)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\n4PqKQ02ihadQFPNJRRSA/9k=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Montaging...\n",
            "...done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpRwPLzvSNDq",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BnBdOmqSNDr",
        "colab_type": "text"
      },
      "source": [
        "### First 500 classes of InceptionV1 as per the hyperparameters defined above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzGXtabiSNDs",
        "colab_type": "text"
      },
      "source": [
        "![](4-features/Predictions-500/montage.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YTZ6mYxSNDs",
        "colab_type": "text"
      },
      "source": [
        "#### Some notable (hand-picked) classes\n",
        "Top to bottom: goldfish, Carassius auratus (2), loggerhead, loggerhead turtle, Caretta caretta (34), king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica (122), bee (310), bakery, bakeshop, bakehouse (416)\n",
        "\n",
        "\n",
        "![](3-features/Predictions-500/002.jpg)\n",
        "![](3-features/Predictions-500/034.jpg)  \n",
        "![](3-features/Predictions-500/122.jpg)\n",
        "![](3-features/Predictions-500/310.jpg)\n",
        "![](3-features/Predictions-500/416.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bcaq8KVgSNDt",
        "colab_type": "text"
      },
      "source": [
        "### Random 100 filters of InceptionV1 layer Mixed_4c_Concatenated as per the hyperparameters defined above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUFQwFsqSNDt",
        "colab_type": "text"
      },
      "source": [
        "![](3-features/4C-100R/montage.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0NF7XjhSNDu",
        "colab_type": "text"
      },
      "source": [
        "### Random 100 filters of InceptionV1 layer Mixed_5b_Concatenated as per the hyperparameters defined above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b6sWnDBSNDv",
        "colab_type": "text"
      },
      "source": [
        "![](3-features/5B-100R/montage.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCaJaJhaSNDv",
        "colab_type": "text"
      },
      "source": [
        "## Bibliography\n",
        "\n",
        "- Dosovitskiy, Alexey, and Thomas Brox. \"Generating Images with Perceptual Similarity Metrics Based on Deep Networks.\" In Advances in Neural Information Processing Systems, 658‚Äì66, 2016. http://papers.nips.cc/paper/6157-generating- images-with-perceptual-similarity-metrics-based-on-deep-networks.\n",
        "- Nguyen, Anh, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. \"Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks.\" In Advances in Neural Information Processing Systems,\n",
        "3387‚Äì95, 2016. http://papers.nips.cc/paper/6519-synthesizing-the-preferred- inputs-for-neurons-in-neural-networks-via-deep-generator-networks.\n",
        "- Nguyen, Anh, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, and Jeff Clune. \"Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space.\" arXiv Preprint, 2017. https://arxiv.org/abs/1612.00005.\n",
        "- Nguyen, Anh, Jason Yosinski, and Jeff Clune. \"Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned by Each Neuron in Deep Neural Networks.\" arXiv Preprint arXiv:1602.03616, 2016. https://arxiv.org/abs/1602.03616.\n",
        "- Olah, Chris, Alexander Mordvintsev, and Ludwig Schubert. \"Feature Visualization.\" Distill, 2017. https://distill.pub/2017/feature-visualization.\n",
        "- Olah, Chris, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. \"The Building Blocks of Interpretability.\" Distill, 2018. https://distill.pub/2018/building-blocks/\n",
        "- Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. \"Deep inside convolutional networks: Visualising image classification models and saliency maps.\" arXiv preprint arXiv:1312.6034, 2013. https://arxiv.org/abs/1312.6034\n",
        "- Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. \"Intriguing Properties of Neural Networks.\" arXiv Preprint arXiv:1312.6199, 2013. https://arxiv.org/abs/1312.6199"
      ]
    }
  ]
}